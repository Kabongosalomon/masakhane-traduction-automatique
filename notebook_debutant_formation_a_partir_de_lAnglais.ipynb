{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igc5itf-xMGj"
   },
   "source": [
    "# Masakhane - Traduction Automatique pour les Langues Africaines (avec JoeyNMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4fXCKCf36IK"
   },
   "source": [
    "## Note avant de commencer :\n",
    "### - L'idée est que vous puissiez y apporter des modifications minimales afin d'obtenir QUELQUES résultats pour votre propre corpus de traduction. \n",
    "\n",
    "### - Le tl;dr : Allez aux commentaires **\"TODO \"** qui vous indiqueront ce qu'il faut mettre à jour pour que tout fonctionne bien.\n",
    "\n",
    "### - Si vous voulez avoir une idée de ce que vous êtes en train de faire, veuillez lire le texte et consulter les liens.\n",
    "\n",
    "### - Avec 100 epochs, l'exécution dans Google Colab devrait prendre environ 7 heures.\n",
    "\n",
    "### - Une fois que vous avez obtenu un bon résultat pour votre langue, veuillez joindre et envoyer par courriel votre notebook qui en est à l'origine à masakhanetranslation@gmail.com.\n",
    "\n",
    "### - Si vous en avez l'occasion, il serait fantastique de faire un bref historique de votre langue. Voir des exemples dans  [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l929HimrxS0a"
   },
   "source": [
    "## Récupérer vos données et créer un corpus parallèle\n",
    "\n",
    "Si vous souhaitez utiliser les données JW300 référencées sur le site de Masakhane ou dans notre repo GitHub, vous pouvez utiliser `opus-tools` pour convertir les données dans un format approprié. `opus_read` de ce package fournit un outil pratique pour lire les fichiers XML natifs alignés et les convertir au format TMX. L'outil peut également être utilisé pour récupérer à la volée des fichiers pertinents d'OPUS ainsi que pour filtrer les données si nécessaire. [Lire la documentation](https://pypi.org/project/opustools-pkg/) pour plus de détails.\n",
    "\n",
    "Une fois que vous avez vos fichiers de corpus au format TMX (une structure XML qui comprend les phrases dans votre langue cible et votre langue source dans un seul fichier), nous recommandons de les lire dans un dataframe Pandas. Heureusement, Jade a écrit un paquet ingénieux `tmx2dataframe` qui convertit votre fichier tmx en un dataframe Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oGRmDELn7Az0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Cn3tgQLzUxwn",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Définissez vos langues source et cible. \n",
    "# N'oubliez pas que ces langues utilisent traditionnellement les codes de langue que vous trouverez ici :\n",
    "# Ces suffixes deviendront également ceux de tous les fichiers de vocabulaire et de corpus utilisés dans l'ensemble du projet.\n",
    "import os\n",
    "source_language = \"en\"\n",
    "target_language = \"xh\" \n",
    "lc   = False      # Si True, mettre les données en minuscules.\n",
    "seed = 42         # Seed aléatoire pour le mélange des données.\n",
    "tag  = \"baseline\" # Donner un nom unique à votre dossier - ceci afin de vous assurer que vous ne réécrivez pas les modèles que vous avez déjà soumis\n",
    "\n",
    "os.environ[\"src\"] = source_language # Les définir en bash également, puisque nous utilisons souvent des scripts bash\n",
    "os.environ[\"tgt\"] = target_language\n",
    "os.environ[\"tag\"] = tag\n",
    "\n",
    "# Ceci va le sauvegarder à la place dans un dossier de notre Gdrive !\n",
    "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
    "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kBSgJHEw7Nvx",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!echo $gdrive_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gA75Fs9ys8Y9",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Installer les outils opus\n",
    "! pip install opustools-pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xq-tDZVks7ZD",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Téléchargement de notre corpus\n",
    "! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
    "\n",
    "# Extraction du fichier du corpus\n",
    "! gunzip JW300_latest_xml_$src-$tgt.xml.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "n48GDRnP8y2G",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Téléchargement de l'ensemble de test global.\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
    "  \n",
    "# And the specific test set for this language pair.\n",
    "os.environ[\"trg\"] = target_language \n",
    "os.environ[\"src\"] = source_language \n",
    "\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
    "! mv test.en-$trg.en test.en\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
    "! mv test.en-$trg.$trg test.$trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NqDG-CI28y2L",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Lecture des données de test à filtrer à partir des splits (lots) train et dev.\n",
    "# Conservation de la portion en anglais dans le lot pour un contrôle rapide du filtrage.\n",
    "en_test_sents = set()\n",
    "filter_test_sents = \"test.en-any.en\"\n",
    "j = 0\n",
    "with open(filter_test_sents) as f:\n",
    "  for line in f:\n",
    "    en_test_sents.add(line.strip())\n",
    "    j += 1\n",
    "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3CNdwLBCfSIl",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Conversion du fichier TMX en dataframe\n",
    "source_file = 'jw300.' + source_language\n",
    "target_file = 'jw300.' + target_language\n",
    "\n",
    "source = []\n",
    "target = []\n",
    "skip_lines = []  # Collecte des numéros de ligne de la partie source pour sauter les mêmes lignes dans la partie cible.\n",
    "with open(source_file) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # Omission des phrases qui sont contenues dans le jeu de test.\n",
    "        if line.strip() not in en_test_sents:\n",
    "            source.append(line.strip())\n",
    "        else:\n",
    "            skip_lines.append(i)             \n",
    "with open(target_file) as f:\n",
    "    for j, line in enumerate(f):\n",
    "        # Ajout au corpus uniquement si la source correspondante n'a pas été omise.\n",
    "        if j not in skip_lines:\n",
    "            target.append(line.strip())\n",
    "    \n",
    "print('Chargement des données et suppression de {}/{} lignes car contenues dans l\\'ensemble de test.'.format(len(skip_lines), i))\n",
    "    \n",
    "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
    "# Si vous obtenez \"TypeError: data argument can't be an iterator\", c'est dû à votre version zip, Exécutez ceci:\n",
    "#df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkuK3B4p2AkN"
   },
   "source": [
    "## Pré-traitement et exportation\n",
    "\n",
    "C'est généralement une bonne idée de supprimer les traductions en double et les traductions contradictoires du corpus. En pratique, ces corpus publics en comportent un certain nombre qui doivent être nettoyés.\n",
    "\n",
    "De plus, nous allons diviser nos données en dev/test/train et les exporter vers le système de fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "M_2ouEOH1_1q",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Suppression des traductions en double\n",
    "df_pp = df.drop_duplicates()\n",
    "\n",
    "# Suppression des traductions contradictoires\n",
    "# (ceci est facultatif et vous pouvez le commenter en \n",
    "# fonction de la taille de votre corpus)\n",
    "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
    "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
    "\n",
    "# Mélange des données pour éliminer les biais dans la sélection des ensembles de données.\n",
    "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Z_1BwAApEtMk",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Installaton de fuzzy wuzzy pour supprimer les phrases\n",
    "# \"presque en double\" dans les jeux de test et d'entraînement.\n",
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein\n",
    "import time\n",
    "from fuzzywuzzy import process\n",
    "import numpy as np\n",
    "from os import cpu_count\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# Réinitialisation de l'index de l'ensemble d'apprentissage après le filtrage précédent\n",
    "df_pp.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# Suppression des échantillons de l'ensemble de données d'apprentissage\n",
    "# s'ils \"se chevauchent presque\" avec les échantillons de l'ensemble de test.\n",
    "\n",
    "# Fonction de filtrage. Ajustement du tampon (pad) pour réduire les correspondances candidates\n",
    "# à une certaine longueur de caractères de l'échantillon donné.\n",
    "def fuzzfilter(sample, candidates, pad):\n",
    "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
    "  if len(candidates) > 0:\n",
    "    return process.extractOne(sample, candidates)[1]\n",
    "  else:\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "### itérer sur les lignes du dataframe pandas n'est pas recommandé, \n",
    "### utilisons le traitement multiple pour appliquer la fonction.\n",
    "\n",
    "with Pool(cpu_count()-1) as pool:\n",
    "    scores = pool.map(partial(fuzzfilter, candidates=list(en_test_sents), pad=5), df_pp['source_sentence'])\n",
    "hours, rem = divmod(time.time() - start_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"done in {}h:{}min:{}seconds\".format(hours, minutes, seconds))\n",
    "\n",
    "# Filtrage des \"échantillons qui se recouvrent quasiment\".\n",
    "df_pp = df_pp.assign(scores=scores)\n",
    "df_pp = df_pp[df_pp['scores'] < 95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hxxBOCA-xXhy",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cette section effectue la répartition entre train/dev pour les corpus parallèles\n",
    "# et les enregistre dans des fichiers séparés.\n",
    "# Nous utilisons 1000 pour dev test et le jeu de test donné.\n",
    "import csv\n",
    "\n",
    "# Do the split between dev/train and create parallel corpora\n",
    "num_dev_patterns = 1000\n",
    "\n",
    "# Facultatif : mettre les corpus en minuscules - cela facilitera la généralisation, mais sans la casse appropriée.\n",
    "if lc:  # Julia: rendre la mise en minuscule facultative\n",
    "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
    "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
    "\n",
    "# Julia: les jeux de test ont déjà été générés\n",
    "dev = df_pp.tail(num_dev_patterns) # Herman: Erreur dans l'original\n",
    "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
    "\n",
    "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in stripped.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "    \n",
    "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in dev.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "\n",
    "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Ajout partout de `header=False`\n",
    "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Traitement problématique des guillemets.\n",
    "\n",
    "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
    "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
    "\n",
    "# Vérification du format ci-dessous. Il ne doit pas y avoir de guillemets supplémentaires ou de caractères bizarres.\n",
    "! head train.*\n",
    "! head dev.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epeCydmCyS8X"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Installation de JoeyNMT\n",
    "\n",
    "JoeyNMT est un paquet NMT (ou TAN - Traduction Automatique Neurale) simple et minimaliste, utile pour l'apprentissage et l'enseignement. Vous pouvez consulter la documentation de JoeyNMT [ici](https://joeynmt.readthedocs.io)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iBRMm4kMxZ8L",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Installation de JoeyNMT\n",
    "! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install .\n",
    "# Installation de Pytorch avec le support GPU v1.7.1.\n",
    "! pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaE77Tcppex9"
   },
   "source": [
    "# Prétraitement des données en jetons (tokens) de sous-mots BPE\n",
    "\n",
    "- L'une des améliorations les plus performantes pour les langues agglutinantes (une caractéristique de la plupart des langues bantoues) est l'utilisation de la tokenisation BPE. [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
    "\n",
    "- Il a également été démontré qu'en optimisant le nombre de codes BPE, nous améliorons considérablement les résultats pour les langues à faibles ressources. [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
    "\n",
    "- Ci-dessous, nous avons les scripts pour faire la tokenisation BPE de nos données. Nous utilisons 4000 tokens comme recommandé par [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). Vous n'avez besoin de rien changer. Il suffit juste d'exécuter le programme ci-dessous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "H-TyjtmXB1mL",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Une des améliorations considérables des performances du NMT a été \n",
    "# l'utilisation d'une méthode différente de tokenisation. \n",
    "# Habituellement, NMT effectue une tokénisation par mots. Cependant, l'utilisation d'une méthode \n",
    "# appelée BPE (Byte Pair Encoding) a permis d'améliorer considérablement les performances.\n",
    "\n",
    "# Effectuer un NMT de sous-mots\n",
    "from os import path\n",
    "os.environ[\"src\"] = source_language # Les définir en bash également, puisque nous utilisons souvent des scripts bash\n",
    "os.environ[\"tgt\"] = target_language\n",
    "\n",
    "# Apprentissage des BPE sur les données de formation.\n",
    "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
    "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
    "\n",
    "# Application des splits (fractions) BPE aux données de développement et de test.\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
    "\n",
    "# Création d'un répertoire et déplacement de toutes les données qui nous intéressent vers le bon emplacement.\n",
    "! mkdir -p $data_path\n",
    "! cp train.* $data_path\n",
    "! cp test.* $data_path\n",
    "! cp dev.* $data_path\n",
    "! cp bpe.codes.4000 $data_path\n",
    "! ls $data_path\n",
    "\n",
    "# Déplacement également de tout ce qui nous intéresse vers un emplacement \n",
    "# monté dans google drive à gdrive_path (pertinent si vous travaillez en colab).\n",
    "! cp train.* \"$gdrive_path\"\n",
    "! cp test.* \"$gdrive_path\"\n",
    "! cp dev.* \"$gdrive_path\"\n",
    "! cp bpe.codes.4000 \"$gdrive_path\"\n",
    "! ls \"$gdrive_path\"\n",
    "\n",
    "# Création du vocabulaire à l'aide de build_vocab\n",
    "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
    "\n",
    "# Quelques résultats\n",
    "! echo \"BPE Xhosa Sentences\"\n",
    "! tail -n 5 test.bpe.$tgt\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "IlMitUHR8Qy-",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Déplacement également de tout ce qui nous intéresse vers un emplacement \n",
    "# monté dans google drive  à gdrive_path (pertinent si vous travaillez en colab).\n",
    "! cp train.* \"$gdrive_path\"\n",
    "! cp test.* \"$gdrive_path\"\n",
    "! cp dev.* \"$gdrive_path\"\n",
    "! cp bpe.codes.4000 \"$gdrive_path\"\n",
    "! ls \"$gdrive_path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ixmzi60WsUZ8"
   },
   "source": [
    "# Création de la configuration de JoeyNMT\n",
    "\n",
    "JoeyNMT nécessite une configuration yaml. Nous fournissons un modèle ci-dessous. \n",
    "Nous avons également défini un certain nombre de valeurs par défaut, avec lesquelles vous pouvez jouer !\n",
    "\n",
    "- Nous avons utilisé l'architecture Transformer \n",
    "- Nous avons fixé notre taux de dropout (décrochage) à un niveau raisonnablement élevé : 0,3 (recommandé dans le document  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
    "\n",
    "Des choses qui valent la peine de jouer avec :\n",
    "- La taille du lot ou batch (qu'il est également recommandé de modifier pour les langues à faibles ressources)\n",
    "- Le nombre d'époques (nous l'avons fixé à 30 pour qu'il s'exécute en une heure environ, à des fins de test).\n",
    "- Les caractéristiques du décodeur (beam_size, alpha)\n",
    "- Les métriques d'évaluation (BLEU versus Crhf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PIs1lY2hxMsl",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Ceci crée le fichier de configuration de notre système JoeyNMT. Étant donné que ce fichier peut sembler \n",
    "# trop volumineux, nous vous fournissons quelques paramètres utiles à mettre à jour.\n",
    "# (Vous pouvez bien sûr jouer avec tous les paramètres si vous le voulez !)\n",
    "\n",
    "name = '%s%s' % (source_language, target_language)\n",
    "gdrive_path = os.environ[\"gdrive_path\"]\n",
    "\n",
    "# Création de la configuration\n",
    "# Note: TODO -> À faire\n",
    "config = \"\"\"\n",
    "name: \"{name}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language}\"\n",
    "    train: \"data/{name}/train.bpe\"\n",
    "    dev:   \"data/{name}/dev.bpe\"\n",
    "    test:  \"data/{name}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"data/{name}/vocab.txt\"\n",
    "    trg_vocab: \"data/{name}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # si décommenté, charger un modèle pré-entraîné à partir de ce point de contrôle (checkpoint)\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: Essaie de basculer du plateau au scheduler (planificateur) Noam\n",
    "    patience: 5                     # Pour le plateau : diminuer le taux d'apprentissage par le decrease_factor (facteur de diminution) si le score de validation ne s'est pas amélioré pendant ce nombre de tours de validation..\n",
    "    learning_rate_factor: 0.5       # Facteur pour l'ordonnanceur Noam (utilisé avec le Transformer)\n",
    "    learning_rate_warmup: 1000      # Étapes de démarrage (warmup steps) pour le scheduler Noam (utilisé avec le Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                     # TODO: Diminuer pour jouer et vérifier le fonctionnement. Une trentaine de minutes suffisent pour vérifier si le système fonctionne.\n",
    "    validation_freq: 1000          # TODO: Défini à au moins une fois par époque (epoch).\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_transformer\"\n",
    "    overwrite: False               # TODO: À mettre à True si vous voulez écraser les modèles éventuellement existants. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Augmenter à 8 pour des données plus importantes.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Augmenter à 512 pour des données plus importantes.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Augmenter à 512 pour des données plus importantes.\n",
    "        ff_size: 1024            # TODO: Augmenter à 2048 pour des données plus importantes.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Augmenter à 8 pour des données plus importantes.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Augmenter à 512 pour des données plus importantes.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Augmenter à 512 pour des données plus importantes.\n",
    "        ff_size: 1024            # TODO: Augmenter à 2048 pour des données plus importantes.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIifxE3Qzuvs"
   },
   "source": [
    "# Formation du modèle\n",
    "\n",
    "Cette seule ligne de joeynmt exécute l'entraînement en utilisant la configuration que nous avons faite ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6ZBPFwT94WpI",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Formation du modèle\n",
    "# Vous pouvez appuyer sur Ctrl-C pour interrompre. Ensuite, exécutez \n",
    "# la cellule suivante pour sauvegarder vos points de contrôle (checkpoints) ! \n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MBoDS09JM807",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Copie des modèles créés depuis le stockage de l'ordinateur portable vers Google Drive pour un stockage persistant. \n",
    "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "n94wlrCjVc17",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sortie de notre précision de validation (validation accuracy)\n",
    "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "66WhRE9lIhoD",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Test du modèle\n",
    "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "starter_notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
